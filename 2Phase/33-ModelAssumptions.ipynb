{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Assumptions of Linear Regression\n",
    "\n",
    "Linear regression models have some underlying assumptions, mostly captured by the following points:\n",
    "\n",
    "1. The true relationship is linear\n",
    "2. No multicollinearity between independent variables\n",
    "3. Errors are normally distributed with mean 0\n",
    "4. Errors are homoskedastic (aka they have constant variance)\n",
    "5. Errors are not correlated (no trends in error terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the office \"you're making some very dangerous assumptions\" gif from gfycat](https://thumbs.gfycat.com/DarkParallelArizonaalligatorlizard-size_restricted.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inputs \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using this basic advertising data...\n",
    "df = pd.read_csv('Data/Advertising.csv', index_col = 0 )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df.drop(columns='Sales')\n",
    "y = df['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Each Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "\n",
    "Why do we assume linearity? Because, by modeling the relationship using _linear_ regression - if we don't think the relationship is linear, we probably should use a different model.\n",
    "\n",
    "I'll note that linear regression can still handle curvature in the relationship using polynomial variables, interaction terms, etc (more on that in Topic 20) - but this assumption captures the idea that linear parameters (aka coefficients) can capture the relationship between X and y.\n",
    "\n",
    "This assumption can be checked by:\n",
    "- using scatterplots - plotting the dependent variable against every independent variable\n",
    "- calculating correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns\n",
    "\n",
    "for x in features:\n",
    "    plt.scatter(X_train[x], y_train)\n",
    "    plt.title(f'Plot of Sales against {x}')\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel('Sales')\n",
    "    plt.show()\n",
    "    \n",
    "# also plot sales against itself\n",
    "plt.scatter(y_train.index, y_train)\n",
    "plt.hlines(y_train.mean(), 0, 200)\n",
    "plt.xlabel('Index Value')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Variance of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quicker solution is to use Seaborn's `pairplot`.  \n",
    "\n",
    "This lets us check for linearity and multicollinearity (the next assumption we'll check) at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a train df so we can see X versus y in our train data\n",
    "train_df = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson's R Value\n",
    "\n",
    "Pearson's R represents a correlation coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check correlations just against sales\n",
    "train_df.corr().Sales.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "AKA when my X variables aren't actually independent - so that a model has trouble determining which change in what X variable is actually influencing `y`.\n",
    "\n",
    "This assumption can be checked by: \n",
    "\n",
    "- using scatterplots - plotting the independent variables against each other (pairplot, above)\n",
    "- calculating correlations (bonus: visualizing them in a heatmap)\n",
    "- calculating their Variable Inflation Factor (VIF) scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly Explore Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check all correlations using the same Pearson's correlation coefficient\n",
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also visualize it\n",
    "ax = sns.heatmap(train_df.corr(), annot=True)\n",
    "# need to manually set my ylim because of my version of matplotlib\n",
    "ax.set_ylim(4, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Variance Inflation Factor (VIF)\n",
    "\n",
    "> \"Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is **equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable**. This ratio is calculated for each independent variable. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model.\"\n",
    "\n",
    "-- Source: https://www.investopedia.com/terms/v/variance-inflation-factor.asp\n",
    "\n",
    "In other words - how well does one of these X variables predict the others?\n",
    "\n",
    "reference: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# defining an empty dataframe to capture the VIF scores\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# For each column,run a variance_inflaction_factor against all other columns to get a VIF Factor score\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(len(X_train.columns))]\n",
    "\n",
    "# label the scores with their related columns\n",
    "vif[\"features\"] = X_train.columns\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Residuals\n",
    "\n",
    "By checking the residuals, aka the error between our actual y values and what we predicted, we can check that:\n",
    "\n",
    "- Errors are normally distributed with mean 0\n",
    "- Errors are homoskedastic (aka they have constant variance)\n",
    "- Errors are not correlated (no trends in error terms)\n",
    "\n",
    "In a nutshell:\n",
    "\n",
    "<img src=\"images/error-dist.jpeg\" width=\"550\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't look at errors/residuals before modeling!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train_preds = lr.predict(X_train)\n",
    "test_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of curiosity, check the score real fast\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f\"Train R2: {r2_score(y_train, train_preds):.4f}\")\n",
    "print(f\"Test R2: {r2_score(y_test, test_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality\n",
    "\n",
    "There are several ways to test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate our residuals\n",
    "train_residuals = y_train - train_preds\n",
    "test_residuals = y_test - test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram of residuals\n",
    "plt.hist(train_residuals, label='Train')\n",
    "plt.hist(test_residuals, label='Test')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ plots are generally great tools for checking for normality.\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(train_residuals, line = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroskedasticity and Lack of Trend in Errors - Use a Residual Plot\n",
    "\n",
    "#### Typical Residuals vs. Predictions plots:\n",
    "\n",
    "- **The ideal scenario**\n",
    "\n",
    "    - Random scatter\n",
    "    - Scattered around 0\n",
    "    - No identifiable trend\n",
    "    \n",
    "    <img src=\"images/normal-resid.png\" width=\"550\">  \n",
    "    \n",
    "- **Non-linear relationship**\n",
    "\n",
    "    - Clear non-linear scatter, but\n",
    "    - Identifiable trend\n",
    "    - **Fix:** Introduce polynomial terms\n",
    "    - **Fix:** Variable transformation\n",
    "    \n",
    "    <img src=\"images/polynomial-resid.png\" width=\"550\">\n",
    "\n",
    "- **Autocorrelation**\n",
    "\n",
    "    - Identifiable trend, or\n",
    "    - Consecutively positive/negative residuals\n",
    "    - **Fix:** Consider sequential analysis methods (which we'll discuss in phase 4)\n",
    "    \n",
    "    <img src=\"images/autocorrelation.png\" width=\"550\">\n",
    "\n",
    "- **Heteroskedasticity**\n",
    "\n",
    "    - The spread of residuals is different at different levels of the fitted values\n",
    "    - **Fix:** Variable transformation (log)  \n",
    "    \n",
    "    <img src=\"images/heteroskedasticity.png\" width=\"550\">\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're just doing/exploring simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that these residplots only work for single variables\n",
    "sns.residplot(x=X_train['TV'], y=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our full model\n",
    "plt.scatter(train_preds, train_residuals, label='Train')\n",
    "plt.scatter(test_preds, test_residuals, label='Test')\n",
    "\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('residuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: a library named [Yellowbrick](https://www.scikit-yb.org/en/latest/index.html) has code for a nice Residuals Plot built out for you (works if you're using a sklearn model) - https://www.scikit-yb.org/en/latest/api/regressor/residuals.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Potential Problems\n",
    "\n",
    "- Outliers\n",
    "\n",
    "    <img src='images/outliers.png' width=450>\n",
    "\n",
    "- High Leverage Points \n",
    "\n",
    "    <img src='images/leverage.png' width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Penn State Stats on Influential Points (outliers, high leverage points)](https://online.stat.psu.edu/stat462/node/87/) - this resource also allows easy access to the rest of their material on regression\n",
    "\n",
    "- [Statsmodels' Documentation: Check the influence of outliers](https://www.statsmodels.org/devel/generated/statsmodels.stats.outliers_influence.OLSInfluence.html)\n",
    "\n",
    "- [Long blog post on regression diagnostics with implementation in python](http://songhuiming.github.io/pages/2016/12/31/linear-regression-in-python-chapter-2/)\n",
    "\n",
    "- [Statistics by Jim: Linear Regression Assumptions](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

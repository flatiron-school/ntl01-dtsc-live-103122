{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.897799Z",
     "start_time": "2023-01-06T17:51:36.642082Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import gauss as gs, uniform as uni, seed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Netflix](https://miro.medium.com/max/1400/1*jlQxemlP9Yim_rWTqlCFDQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Amazon](images/amazon_recommender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe the difference between content-based and collaborative-filtering algorithms\n",
    "- explain and use the cosine similarity metric\n",
    "- describe the algorithm of alternating least-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems can be classified along various lines. One fundamental distinction is **content-based** vs. **collaborative-filtering** systems.\n",
    "\n",
    "To illustrate this, consider two different strategies: (a) I recommend items to you that are *similar to other items* you've used/bought/read/watched; and (b) I recommend items to you that people *similar to you* have used/bought/read/watched. The first is the **content-based strategy**; the second is the **collaborative-filtering strategy**. \n",
    "\n",
    "Another distinction drawn is in whether (a) the system uses existing ratings to compute user-user or item-item similarity, or (b) the system uses machine learning techniques to make predictions. Recommenders of the first sort are called **memory-based**; recommenders of the second sort are called **model-based**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea here is to recommend items to a user that are *similar to* items that the user has already enjoyed. Suppose we represent TV shows as rows, where the columns represent various features of these TV shows. These features might be things like the presence of a certain actor or the show fitting into a particular genre etc. We'll just use binary features here, perhaps the result of some one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.913294Z",
     "start_time": "2023-01-06T17:51:37.899294Z"
    }
   },
   "outputs": [],
   "source": [
    "tv_shows = np.array([[0, 1, 1, 0, 1, 1, 1],\n",
    "                   [0, 0, 0, 1, 1, 1, 0],\n",
    "                   [1, 1, 1, 0, 0, 1, 1],\n",
    "                   [0, 1, 1, 1, 0, 0, 1]])\n",
    "\n",
    "tv_shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob likes the TV Show represented by Row \\#1. Which show (row) should we recommend to Bob?\n",
    "\n",
    "One natural way of measuring the similarity between two vectors is by the **cosine of the angle between them**. Two points near one another in feature space will correspond to vectors that nearly overlap, i.e. vectors that describe a small angle $\\theta$. And as $\\theta$ decreases, $\\cos(\\theta)$ *increases*. So we'll be looking for large values of the cosine (which ranges between -1 and 1). We can also think of the cosine between two vectors as the *projection of one vector onto the other*:\n",
    "\n",
    "![image.png](https://www.oreilly.com/api/v2/epubs/9781788295758/files/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png)\n",
    "We can use this metric easily if we treat our rows (the items we're comparing for similarity) as vectors: We can calculate the cosine of the angle $\\theta$ between two vectors $\\vec{a}$ and $\\vec{b}$ as follows: $\\cos(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.928794Z",
     "start_time": "2023-01-06T17:51:37.914794Z"
    }
   },
   "outputs": [],
   "source": [
    "numerators = np.array([tv_shows[0].dot(tv_show) for tv_show in tv_shows[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.944293Z",
     "start_time": "2023-01-06T17:51:37.930294Z"
    }
   },
   "outputs": [],
   "source": [
    "numerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.959793Z",
     "start_time": "2023-01-06T17:51:37.945294Z"
    }
   },
   "outputs": [],
   "source": [
    "denominators = np.array(\n",
    "    [(np.linalg.norm(tv_shows[0]) * np.linalg.norm(tv_show)) for tv_show in tv_shows[1:]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.975293Z",
     "start_time": "2023-01-06T17:51:37.961294Z"
    }
   },
   "outputs": [],
   "source": [
    "denominators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:37.990794Z",
     "start_time": "2023-01-06T17:51:37.976293Z"
    }
   },
   "outputs": [],
   "source": [
    "numerators / denominators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cosine similarity to Row \\#1 is highest for Row \\#3, we would recommend this TV show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Now the idea is to recommend items to a user based on what *similar* users have enjoyed. Suppose we have the following recording of explicit ratings of five items by three users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.006293Z",
     "start_time": "2023-01-06T17:51:37.993293Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = np.array([[5, 4, 3, 4, 5], [3, 1, 1, 2, 5], [4, 2, 3, 1, 4]])\n",
    "\n",
    "new_user = np.array([5, 0, 0, 0, 0])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which user is `new_user` most similar?\n",
    "\n",
    "One metric is cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.021794Z",
     "start_time": "2023-01-06T17:51:38.007794Z"
    }
   },
   "outputs": [],
   "source": [
    "new_user_mag = 5\n",
    "\n",
    "numerators = np.array([new_user.dot(user) for user in users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.037293Z",
     "start_time": "2023-01-06T17:51:38.022794Z"
    }
   },
   "outputs": [],
   "source": [
    "numerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.052798Z",
     "start_time": "2023-01-06T17:51:38.038796Z"
    }
   },
   "outputs": [],
   "source": [
    "denominators = np.array(\n",
    "    [(np.linalg.norm(new_user) * np.linalg.norm(user)) for user in users]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.068301Z",
     "start_time": "2023-01-06T17:51:38.054294Z"
    }
   },
   "outputs": [],
   "source": [
    "denominators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.083794Z",
     "start_time": "2023-01-06T17:51:38.069793Z"
    }
   },
   "outputs": [],
   "source": [
    "numerators / denominators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we could also use another metric, such as Pearson Correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.114793Z",
     "start_time": "2023-01-06T17:51:38.085794Z"
    }
   },
   "outputs": [],
   "source": [
    "[np.corrcoef(new_user, user)[0, 1] for user in users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on content-based vs. collaborative systems, see [this Wikipedia article](https://en.wikipedia.org/wiki/Collaborative_filtering) and [this blog post](https://towardsdatascience.com/recommendation-systems-models-and-evaluation-84944a84fb8e). [This post](https://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/) on dataconomy is also useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "Suppose we start with a matrix $R$ of users and products, where each cell records the ranking the relevant user gave to the relevant product. Very often we'll be able to record this data as a sparse matrix, because many users will not have ranked many items.\n",
    "\n",
    "Imagine factoring this matrix into a user matrix $P$ and an item matrix $Q^T$: $R = PQ^T$. What would the shapes of $P$ and $Q^T$ be? Clearly $P$ must have as many rows as $R$, which is just the number of users who have given ratings. Similarly, $Q^T$ must have as many columns as $R$, which is just the number of items that have received ratings. We also know that the number of columns of $P$ must match the number of rows of $Q^T$ for the factorization to be possible, but this number could really be anything. In practice this will be a small number, and for reasons that will emerge shortly let's refer to these dimensions as **latent features** of the items in $R$. If $p$ is a row of $P$, i.e. a user-vector, and $q$ is a column of $Q^T$, i.e. an item-vector, then $p$ will record the user's particular weights or *preferences* with respect to the latent features, while $q$ will record how the item ranks with respect to those same latent features. This in turn means that we could predict a user's ranking of a particular item simply by calculating the dot-product of $p$ and $q$! \n",
    "\n",
    "If we could effect such a factorization, $R = PQ^T$, then we could calculate *all* predictions, i.e. fill in the gaps in $R$, by solving for $P$ and $Q$.\n",
    "\n",
    "The isolation of these latent features can be achieved in various ways. But this is at heart a matter of **dimensionality reduction**, and so one way is with the [SVD](https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75).\n",
    "\n",
    "An alternative is to use the method of Alternating Least Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least-Squares (ALS)\n",
    "\n",
    "ALS recommendation systems are often implemented in Spark architectures because of the appropriateness for distributed computing. ALS systems often involve very large datasets (consider how much data the recommendation engine for NETFLIX must have, for example!), and it is often useful to store them as sparse matrices, which Spark's ML library can handle. In fact, Spark's mllib even includes a \"Rating\" datatype! ALS is **collaborative** and **model-based**, and is especially useful for working with *implicit* ratings.\n",
    "\n",
    "We're looking for two matrices (a user matrix and an item matrix) into which we can factor our ratings matrix. We can't of course solve for two matrices at once. But here's what we can do:\n",
    "\n",
    "Make guesses of the values for $P$ and $Q$. Then hold the values of one *constant* so that we can optimize for the values of the other!\n",
    "\n",
    "Basically this converts our problem into a familiar *least-squares* problem. See [this page](https://textbooks.math.gatech.edu/ila/least-squares.html) and [this page](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/) for more details, but here's the basic idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have an equation $Ax = b$ for *non-square* $A$, then we have:\n",
    "\n",
    "$A^TAx = A^Tb$ <br/>\n",
    "Thus: <br/>\n",
    "$x = (A^TA)^{-1}A^Tb$\n",
    "\n",
    "This $(A^TA)^{-1}A^T$ **is the pseudo-inverse of** $A$. We encountered this before in our whirlwind tour of linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.130293Z",
     "start_time": "2023-01-06T17:51:38.116796Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "A = np.random.rand(5, 5)\n",
    "b = np.random.rand(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.145793Z",
     "start_time": "2023-01-06T17:51:38.131793Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(A.T.dot(A)).dot(A.T).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy` library has a shortcut for this: `numpy.linalg.pinv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.161293Z",
     "start_time": "2023-01-06T17:51:38.147294Z"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.pinv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When we talk about collaborative filtering for recommender systems we want to solve the problem of our original matrix having millions of different dimensions, but our 'tastes' not being nearly as complex. Even if iâ€™ve \\[sic\\] viewed hundreds of items they might just express a couple of different tastes. Here we can actually use matrix factorization to mathematically reduce the dimensionality of our original 'all users by all items' matrix into something much smaller that represents 'all items by some taste dimensions' and 'all users by some taste dimensions'. These dimensions are called ***latent or hidden features*** and we learn them from our data\" ([Medium article: \"ALS Implicit Collaborative Filtering\"](https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Example\n",
    "\n",
    "Suppose Max and Erin have rated five films:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.176794Z",
     "start_time": "2023-01-06T17:51:38.162794Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_arr = pd.DataFrame([[0, 1, 0, 0, 4], [0, 0, 0, 5, 0]],\\\n",
    "                           index=['max', 'erin'],\n",
    "             columns=['film' + str(i) for i in range(1, 6)])\n",
    "ratings_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that we isolate ten latent features of these films, and that we can capture our users, i.e. the tastes of Max and Erin, according to these features. (We'll just fill out a matrix randomly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.192293Z",
     "start_time": "2023-01-06T17:51:38.178294Z"
    }
   },
   "outputs": [],
   "source": [
    "seed(100)\n",
    "users = []\n",
    "\n",
    "for _ in range(2):\n",
    "    user = []\n",
    "    for _ in range(10):\n",
    "        user.append(gs(0, 1))\n",
    "    users.append(user)\n",
    "users_arr = np.array(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.207794Z",
     "start_time": "2023-01-06T17:51:38.194294Z"
    }
   },
   "outputs": [],
   "source": [
    "users_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make another random matrix that expresses *our items* in terms of these latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.223293Z",
     "start_time": "2023-01-06T17:51:38.209294Z"
    }
   },
   "outputs": [],
   "source": [
    "seed(100)\n",
    "items = []\n",
    "\n",
    "for _ in range(5):\n",
    "    item = []\n",
    "    for _ in range(10):\n",
    "        item.append(gs(0, 1))\n",
    "    items.append(item)\n",
    "items_arr = np.array(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.238795Z",
     "start_time": "2023-01-06T17:51:38.225294Z"
    }
   },
   "outputs": [],
   "source": [
    "items_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.254142Z",
     "start_time": "2023-01-06T17:51:38.242793Z"
    }
   },
   "outputs": [],
   "source": [
    "users_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.269293Z",
     "start_time": "2023-01-06T17:51:38.256794Z"
    }
   },
   "outputs": [],
   "source": [
    "items_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct our large users-by-items matrix, we'll simply take the product of our two random matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.284794Z",
     "start_time": "2023-01-06T17:51:38.270795Z"
    }
   },
   "outputs": [],
   "source": [
    "users_arr.dot(items_arr.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's where the ALS really kicks in: We'll solve for Max's and Erin's preference vectors by multiplying the pseudo-inverse of the items array by their respective ratings vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x = (A^TA)^{-1}A^Tb$\n",
    "\n",
    " $MaxPreferences = (items^Titems)^{-1}items^TMaxRatings$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.300295Z",
     "start_time": "2023-01-06T17:51:38.286795Z"
    }
   },
   "outputs": [],
   "source": [
    "max_pref = np.linalg.pinv(items_arr).dot(ratings_arr.loc['max', :])\n",
    "print(max_pref)\n",
    "erin_pref = np.linalg.pinv(items_arr).dot(ratings_arr.loc['erin', :])\n",
    "print(erin_pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.315794Z",
     "start_time": "2023-01-06T17:51:38.301794Z"
    }
   },
   "outputs": [],
   "source": [
    "items_arr.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll predict (or, in this case, reproduce) Max's and Erin's ratings for films by simply multiplying their preference vectors by the transpose of the items array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.331294Z",
     "start_time": "2023-01-06T17:51:38.318295Z"
    }
   },
   "outputs": [],
   "source": [
    "newmax = max_pref.dot(items_arr.T)\n",
    "newmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines up with the ratings with which we began."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.346794Z",
     "start_time": "2023-01-06T17:51:38.333294Z"
    }
   },
   "outputs": [],
   "source": [
    "newerin = erin_pref.dot(items_arr.T)\n",
    "newerin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T20:23:29.898033Z",
     "start_time": "2023-01-06T20:23:29.891535Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ditto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a quick error calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.362294Z",
     "start_time": "2023-01-06T17:51:38.348794Z"
    }
   },
   "outputs": [],
   "source": [
    "guess = np.vstack([newmax, newerin])\n",
    "\n",
    "err = 0\n",
    "for i in range(2):\n",
    "    for j in range(len(ratings_arr.values[i, :])):\n",
    "        if ratings_arr.values[i, j] != 0:\n",
    "            err += (ratings_arr.values[i, j] - guess[i, j])**2\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.377794Z",
     "start_time": "2023-01-06T17:51:38.363794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Users: m x n (m users)\n",
    "# Items: r x n (r items)\n",
    "# Ratings: m x r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.408794Z",
     "start_time": "2023-01-06T17:51:38.379795Z"
    }
   },
   "outputs": [],
   "source": [
    "# If P = users and Q = items, then we want to approximate R = PQ^T\n",
    "# Let's generate R.\n",
    "\n",
    "seed(42)\n",
    "ratings2 = []\n",
    "for _ in range(100):\n",
    "    user = []\n",
    "    for _ in range(100):\n",
    "        chance = gs(0, 0.4)\n",
    "        \n",
    "        # We'll fill our ratings matrix mostly with 0's to represent\n",
    "        # unrated films. This is NOT standard; we're doing this only\n",
    "        # to illustrate the general algorithm.\n",
    "        if chance > 0.5:\n",
    "            user.append(int(uni(1, 6)))\n",
    "        else:\n",
    "            user.append(0)\n",
    "        \n",
    "        # This 'if' will simply ensure that everyone has given at least\n",
    "        # one rating.\n",
    "        if user.count(0) == 10:\n",
    "            user[int(uni(0, 10))] = int(uni(1, 6))\n",
    "    ratings2.append(user)\n",
    "ratings_arr2 = np.array(ratings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T20:24:41.364374Z",
     "start_time": "2023-01-06T20:24:41.345874Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.424293Z",
     "start_time": "2023-01-06T17:51:38.410294Z"
    }
   },
   "outputs": [],
   "source": [
    "users2 = []\n",
    "\n",
    "# Random generation of values for the user matrix\n",
    "for _ in range(100):\n",
    "    user = []\n",
    "    for _ in range(10):\n",
    "        user.append(gs(0, 1))\n",
    "    users2.append(user)\n",
    "users_arr2 = np.array(users2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T20:24:49.335441Z",
     "start_time": "2023-01-06T20:24:49.330443Z"
    }
   },
   "outputs": [],
   "source": [
    "users_arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.439795Z",
     "start_time": "2023-01-06T17:51:38.426294Z"
    }
   },
   "outputs": [],
   "source": [
    "items2 = []\n",
    "\n",
    "# Random generation of values for the item matrix\n",
    "for _ in range(100):\n",
    "    item = []\n",
    "    for _ in range(10):\n",
    "        item.append(gs(0, 1))\n",
    "    items2.append(item)\n",
    "items_arr2 = np.array(items2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T20:24:58.744006Z",
     "start_time": "2023-01-06T20:24:58.730506Z"
    }
   },
   "outputs": [],
   "source": [
    "items_arr2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first guess at filling in the matrix will simply be the matrix product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.455294Z",
     "start_time": "2023-01-06T17:51:38.441294Z"
    }
   },
   "outputs": [],
   "source": [
    "guess = users_arr2.dot(items_arr2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.470793Z",
     "start_time": "2023-01-06T17:51:38.456794Z"
    }
   },
   "outputs": [],
   "source": [
    "guess.shape == ratings_arr2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a measure of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.486293Z",
     "start_time": "2023-01-06T17:51:38.472293Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_arr2 - guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T17:51:38.501793Z",
     "start_time": "2023-01-06T17:51:38.487793Z"
    }
   },
   "outputs": [],
   "source": [
    "err = (ratings_arr2 - guess)**2\n",
    "\n",
    "np.sum(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty terrible! But we started with random numbers and have only done one iteration. Let's see if we can do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T19:57:35.728097Z",
     "start_time": "2023-01-06T19:57:35.708128Z"
    }
   },
   "outputs": [],
   "source": [
    "def als(ratings, users, items, reps=10):\n",
    "    \n",
    "    ratings_cols = ratings.T\n",
    "    for _ in range(reps):\n",
    "        new_users = []\n",
    "        for i in range(len(ratings)):\n",
    "            \n",
    "            user = LinearRegression(fit_intercept=False)\\\n",
    "            .fit(items, ratings[i]).coef_\n",
    "            new_users.append(user)\n",
    "        new_users = np.asarray(new_users)\n",
    "        \n",
    "        new_items = []\n",
    "        for i in range(len(ratings)):\n",
    "            \n",
    "            item = LinearRegression(fit_intercept=False)\\\n",
    "            .fit(new_users, ratings_cols[i]).coef_\n",
    "            new_items.append(item)\n",
    "        new_items = np.asarray(new_items)\n",
    "        \n",
    "        guess = new_users.dot(new_items.T)\n",
    "        err = 0\n",
    "        for i in range(len(ratings)):\n",
    "            for j in range(len(ratings[i])):\n",
    "                if ratings[i, j] != 0:\n",
    "                    err += (ratings[i, j] - guess[i, j])**2\n",
    "        print(err)\n",
    "        \n",
    "        items = new_items\n",
    "        \n",
    "    return new_users.dot(new_items.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see our error decrease with more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T20:26:44.622763Z",
     "start_time": "2023-01-06T20:26:36.861286Z"
    }
   },
   "outputs": [],
   "source": [
    "new_users2 = als(ratings_arr2, users_arr2, items_arr2,100)[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-06T20:27:02.161912Z",
     "start_time": "2023-01-06T20:27:02.143386Z"
    }
   },
   "outputs": [],
   "source": [
    "new_users2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALS in `pyspark`\n",
    "\n",
    "We'll talk about Big Data and Spark soon, but I'll just note here that Spark has a recommendation submodule inside its ml (machine learning) module. Source code for `pyspark`'s version [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

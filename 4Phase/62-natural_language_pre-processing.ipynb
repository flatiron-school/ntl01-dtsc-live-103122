{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Natural-Language-Pre-Processing\" data-toc-modified-id=\"Natural-Language-Pre-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Natural Language Pre-Processing</a></span></li><li><span><a href=\"#Learning-Goals\" data-toc-modified-id=\"Learning-Goals-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Learning Goals</a></span></li><li><span><a href=\"#Overview-of-NLP\" data-toc-modified-id=\"Overview-of-NLP-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Overview of NLP</a></span></li><li><span><a href=\"#Preprocessing-for-NLP\" data-toc-modified-id=\"Preprocessing-for-NLP-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preprocessing for NLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Tokenization</a></span></li></ul></li><li><span><a href=\"#Text-Cleaning\" data-toc-modified-id=\"Text-Cleaning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Text Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Capitalization\" data-toc-modified-id=\"Capitalization-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Capitalization</a></span></li><li><span><a href=\"#Punctuation\" data-toc-modified-id=\"Punctuation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Punctuation</a></span></li><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Stopwords</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Numerals\" data-toc-modified-id=\"Numerals-5.3.0.1\"><span class=\"toc-item-num\">5.3.0.1&nbsp;&nbsp;</span>Numerals</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Regex\" data-toc-modified-id=\"Regex-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Regex</a></span><ul class=\"toc-item\"><li><span><a href=\"#RegexpTokenizer()\" data-toc-modified-id=\"RegexpTokenizer()-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span><code>RegexpTokenizer()</code></a></span></li></ul></li><li><span><a href=\"#Exercise:-NL-Pre-Processing\" data-toc-modified-id=\"Exercise:-NL-Pre-Processing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Exercise: NL Pre-Processing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to install nltk if needed\n",
    "# !pip install nltk\n",
    "# !conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to download the stopwords if you haven't already - only ever needs to be run once\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Learning Goals\n",
    "\n",
    "- Describe the basic concepts of NLP\n",
    "- Use pre-processing methods for NLP\n",
    "    - Tokenization\n",
    "    - Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Overview of NLP\n",
    "\n",
    "NLP allows computers to interact with text data in a structured and sensible way. In short, we will be breaking up series of texts into individual words (or groups of words), and isolating the words with **semantic value**.  We will then compare texts with similar distributions of these words, and group them together.\n",
    "\n",
    "In this section, we will discuss some steps and approaches to common text data analytic procedures. Some of the applications of natural language processing are:\n",
    "- Chatbots \n",
    "- Speech recognition and audio processing \n",
    "- Classifying documents \n",
    "\n",
    "Here is an example that uses some of the tools we use in this notebook.  \n",
    "  -[chicago_justice classifier](https://github.com/chicago-justice-project/article-tagging/blob/master/lib/notebooks/bag-of-words-count-stemmed-binary.ipynb)\n",
    "\n",
    "We will introduce you to the preprocessing steps, feature engineering, and other steps you need to take in order to format text data for machine learning tasks. \n",
    "\n",
    "We will also introduce you to [**NLTK**](https://www.nltk.org/) (Natural Language Toolkit), which will be our main tool for engaging with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/nlp_process.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal when pre-processing text data for NLP is to remove as many unnecessary words as possible while preserving as much semantic meaning as possible. This will improve your model performance dramatically.\n",
    "\n",
    "You can think of this sort of like dimensionality reduction. The unique words in your corpus form a **vocabulary**, and each word in your vocabulary is essentially another feature in your model. So we want to get rid of unnecessary words and consolidate words that have similar meanings.\n",
    "\n",
    "We will be working with a dataset which includes both satirical** (The Onion) and real news (Reuters) articles. We refer to the entire set of articles as the **corpus**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![the_onion](images/the_onion.jpeg) ![reuters](images/reuters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our goal is to detect satire, so our target class of 1 is associated with The Onion articles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus.loc[10].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus.loc[10].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus.loc[502].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus.loc[502].target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each article in the corpus is refered to as a **document**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is a balanced dataset with 500 documents of each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Discussion:** Let's think about the use cases of being able to correctly separate satirical from authentic news. What might be a real-world use case?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Thoughts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tokenization \n",
    "\n",
    "In order to convert the texts into data suitable for machine learning, we need to break down the documents into smaller parts. \n",
    "\n",
    "The first step in doing that is **tokenization**.\n",
    "\n",
    "Tokenization is the process of splitting documents into units of observations. We usually represent the tokens as __n-grams__, where n represent the number of consecutive words occuring in a document that we will consider a unit. In the case of unigrams (one-word tokens), the sentence \"David works here\" would be tokenized into:\n",
    "\n",
    "- \"David\", \"works\", \"here\";\n",
    "\n",
    "If we want (also) to consider bigrams, we would (also) consider:\n",
    "\n",
    "- \"David works\" and \"works here\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's consider a particular document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_document = corpus.iloc[1].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are many ways to tokenize our document. \n",
    "\n",
    "It is a long string, so the first way we might consider is to split it by spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Knowledge Check:** How would we split our documents into words using spaces?\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "<details>\n",
    "    <summary><b><u>Click Here for Answer Code</u></b></summary>\n",
    "\n",
    "    sample_document.split(' ')\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But this is not ideal. We are trying to create a set of tokens with **high semantic value**.  In other words, we want to isolate text which best represents the meaning in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text Cleaning\n",
    "\n",
    "Most NL Pre-Processing will include the following tasks:\n",
    "\n",
    "  1. Remove capitalization  \n",
    "  2. Remove punctuation  \n",
    "  3. Remove stopwords  \n",
    "  4. Remove numerals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We could manually perform all of these tasks with string operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Capitalization\n",
    "\n",
    "When we create our matrix of words associated with our corpus, **capital letters** will mess things up.  The semantic value of a word used at the beginning of a sentence is the same as that same word in the middle of the sentence.  In the two sentences:\n",
    "\n",
    "sentence_one =  \"Excessive gerrymandering in small counties suppresses turnout.\"   \n",
    "sentence_two =  \"Turnout is suppressed in small counties by excessive gerrymandering.\"  \n",
    "\n",
    "'excessive' has the same semantic value, but will be treated as different tokens because of capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence_one =  \"Excessive gerrymandering in small counties suppresses turnout.\" \n",
    "sentence_two =  \"Turnout is suppressed in small counties by excessive gerrymandering.\"\n",
    "\n",
    "Excessive = sentence_one.split(' ')[0]\n",
    "excessive = sentence_two.split(' ')[-2]\n",
    "print(excessive, Excessive)\n",
    "excessive == Excessive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "manual_cleanup = [word.lower() for word in sample_document.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Our initial token set for our sample document is {len(manual_cleanup)} words long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Our initial token set for our sample document has \\\n",
    "{len(set(sample_document.split(' ')))} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"After removing capitals, our sample document has \\\n",
    "{len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Punctuation\n",
    "\n",
    "Like capitals, splitting on white space will create tokens which include punctuation that will muck up our semantics.  \n",
    "\n",
    "Returning to the above example, 'gerrymandering' and 'gerrymandering.' will be treated as different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "no_punct = sentence_one.split(' ')[1]\n",
    "punct = sentence_two.split(' ')[-1]\n",
    "print(no_punct, punct)\n",
    "no_punct == punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Manual removal of punctuation\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "manual_cleanup = [s.translate(str.maketrans('', '', string.punctuation))\\\n",
    "                  for s in manual_cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"After removing punctuation, our sample document has \\\n",
    "{len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "manual_cleanup[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Stopwords are the **filler** words in a language: prepositions, articles, conjunctions. They have low semantic value, and often need to be removed.  \n",
    "\n",
    "Luckily, NLTK has lists of stopwords ready for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopwords.words('greek')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see which stopwords are present in our sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stops = [token for token in manual_cleanup if token in stopwords.words('english')]\n",
    "stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'There are {len(stops)} instances of {len(set(stops))} \\\n",
    "stopwords in the sample document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'The {len(stops)} instances make up \\\n",
    "{len(stops)/len(manual_cleanup): 0.2%} of our text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's also use the **FreqDist** tool to look at the makeup of our text before and after removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "manual_cleanup = [token for token in manual_cleanup if\\\n",
    "                  token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "manual_cleanup[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We can also customize our stopwords list\n",
    "\n",
    "custom_sw = stopwords.words('english')\n",
    "custom_sw.extend([\"i'd\",\"say\"] )\n",
    "custom_sw[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "manual_cleanup = [token for token in manual_cleanup if token not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'After removing stopwords, there are {len(set(manual_cleanup))} unique words left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Numerals\n",
    "\n",
    "Numerals also usually have low semantic value. Their removal can help improve our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "manual_cleanup = [s.translate(str.maketrans('', '', '0123456789')) \\\n",
    "                  for s in manual_cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop empty strings\n",
    "\n",
    "manual_cleanup = [s for s in manual_cleanup if s != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'After removing numerals, there are {len(set(manual_cleanup))} unique words left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regex\n",
    "\n",
    "Regex allows us to match strings based on a pattern.  This pattern comes from a language of identifiers, which we can begin exploring on the cheatsheet found here:\n",
    "  -   https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A few key symbols:\n",
    "  - . : matches any character\n",
    "  - \\d, \\w, \\s : represent digit, word, whitespace  \n",
    "  - *, ?, +: matches 0 or more, 0 or 1, 1 or more of the preceding character  \n",
    "  - [A-Z]: matches any capital letter  \n",
    "  - [a-z]: matches lowercase letter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Other helpful resources:\n",
    "  - https://regexcrossword.com/\n",
    "  - https://www.regular-expressions.info/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use regex to isolate numerals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pattern = '[0-9]'\n",
    "number = re.findall(pattern, sample_document)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pattern2 = '[0-9]+'\n",
    "number2 = re.findall(pattern2, sample_document)\n",
    "number2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## `RegexpTokenizer()`\n",
    "\n",
    "Sklearn and NLTK provide us with a suite of **tokenizers** for our text preprocessing convenience. So we don't have to do this all by hand every time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remember that the '?' indicates 0 or 1!\n",
    "\n",
    "re.findall(r\"([a-zA-Z]+(?:'[a-z]+)?)\", \"I'd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "sample_doc = tokenizer.tokenize(sample_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_doc = [token.lower() for token in sample_doc]\n",
    "sample_doc = [token for token in sample_doc if token not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_doc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'We are down to {len(set(sample_doc))} unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exercise: NL Pre-Processing\n",
    "\n",
    "**Activity:** Use what you've learned to preprocess the fourth article. How does the length and number of unique words in the article change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

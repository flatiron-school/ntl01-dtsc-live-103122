{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Of Matrix Inverses and Pseudo-Inverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as numbers have multiplicative inverses, so some matrices have matrix inverses. A multiplicative inverse for a number $x$, $x^{-1}$, satisfies the following:\n",
    "\n",
    "$xx^{-1} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x * x**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, an inverse for a matrix $M$, $M^{-1}$, satisfies:\n",
    "\n",
    "- $MM^{-1} = I$\n",
    "- $M^{-1}M = I$,\n",
    "\n",
    "where $I$ is the identity matrix with 1's down the main diagonal and 0's everywhere else:\n",
    "\n",
    "$I = \\begin{bmatrix}1 & 0 & ... & 0 & 0 \\\\\n",
    "0 & 1 & ... & 0 & 0 \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "0 & 0 & ... & 1 & 0 \\\\\n",
    "0 & 0 & ... & 0 & 1 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.random.rand(10, 10)\n",
    "np.round(M.dot(np.linalg.inv(M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we have a numerical equation like $ax = b$, we can represent the solution to this equation as:\n",
    "\n",
    "$x = a^{-1}b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "a = np.random.rand(1)\n",
    "x = np.random.rand(1)\n",
    "b = a*x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**(-1) * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we have a matrix equation like $A\\vec{x} = \\vec{b}$, we can represent the solution to this equation as:\n",
    "\n",
    "$\\vec{x} = A^{-1}\\vec{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.rand(10, 10)\n",
    "x = np.random.rand(10)\n",
    "b = A.dot(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix has an inverse *only if it is square*, and therefore an equation like $\\vec{x} = A^{-1}\\vec{b}$ corresponds to a situation where we have exactly as many rows as we have columns in our dataset. The vector $\\vec{x}$, moreover, represents an exact solution to the system of equations represented by $A$ and $b$.\n",
    "\n",
    "But of course in the typical situation in data science we have more rows than columns (more observations than features), and so we are in the realm not of exact solutions but of optimizations: $A\\vec{x}\\approx\\vec{b}$, for some *non-square* $A$. And the least-squares regression hyperplane, for example, provides exactly that: It's the hyperplane $\\vec{x}$ that minimizes the sum of squared differences between $\\vec{b}$ (our target) and $A\\vec{x}$ (our target estimates).\n",
    "\n",
    "How can we express our optimizing betas $\\vec{x}$ in terms of $A$ and $\\vec{b}$? Observe the following:\n",
    "\n",
    "$A\\vec{x} = \\vec{b}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$A^TA\\vec{x} = A^T\\vec{b}$.\n",
    "\n",
    "Now *this* matrix, $A^TA$, *is* square and so we'll assume that it has an inverse.\n",
    "\n",
    "Thus we write:\n",
    "\n",
    "$(A^TA)^{-1}(A^TA)\\vec{x} = (A^TA)^{-1}A^T\\vec{b}$, i.e.\n",
    "\n",
    "$\\vec{x} = (A^TA)^{-1}A^T\\vec{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analogy with the situation with square $A$, this matrix $(A^TA)^{-1}A^T$ is called the ***pseudo-inverse*** of $A$: Just as $A^{-1}\\vec{b}$ provides an exact solution for $\\vec{x}$ in the equation $A\\vec{x} = \\vec{b}$ (square $A$), so $(A^TA)^{-1}A^T\\vec{b}$ provides the solution to the least-squares optimization problem $A\\vec{x}\\approx\\vec{b}$ (non-square $A$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.rand(100, 10)\n",
    "x = np.random.rand(10)\n",
    "b = A.dot(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A.T.dot(A)).dot(A.T).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy has a shortcut for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.pinv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this matrix calculation with the betas found by `sklearn.linear_model.LinearRegression()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=False).fit(A, b).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
